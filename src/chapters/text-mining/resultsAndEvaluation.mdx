<article id="resultsAndEvaluation">
  <header style={{marginBottom: "1rem"}}>
    ## Final Summary üèÅ
  </header>

In this project, we tried to understand the factors that make top-rated games different from worst-rated games based on user reviews. We used various analyses, like text preprocessing, sentiment analysis, and model building, to find key insights and patterns in the data.

### Key Findings

1. **Bigram Comparison**:
   - **Top-Rated Games**:
     - Positive reviews often mentioned recommendations, saying the game is fun, engaging, and the best in its category. This showed the overall enjoyment and satisfaction players felt.
     - Negative reviews pointed to specific issues within the game, focusing on particular problems even though the game had a high rating.
   - **Worst-Rated Games**:
     - Positive reviews were more general and sometimes mentioned issues like poor optimization, appearance, or functionality. These reviews suggested that any positive aspects were overshadowed by significant flaws.
     - Negative reviews often discussed the drawbacks of early access, wasted money, and time, showing players' frustrations and disappointments.

2. **Review Length and Sentiment**:
   - **Top-Rated Games**: Positive reviews were usually shorter and more to the point, while negative reviews were longer and more detailed, indicating that players felt the need to explain the issues.
   - **Worst-Rated Games**: Negative reviews were also longer and more detailed, showing significant dissatisfaction, while positive reviews were fewer and less enthusiastic.

### Summary of Model Results

We built and evaluated several models to classify the reviews, using different text representation techniques.

- **Bag-of-Words (BoW)**:
  - **Accuracy:** 85.09% without normalization; 81.73% with StandardScaler. Normalization did not help and might have caused a loss of information.
  - **Iterations:** Converged after 216 iterations.

- **TF-IDF**:
  - **Accuracy:** 85.74% without normalization; 80.46% with StandardScaler. Like BoW, standardization reduced accuracy.

- **Word2Vec**:
  - **Accuracy:** 58.30%. While lower in accuracy, Word2Vec offers deeper insights into word meanings and relationships in reviews.

### Conclusion

Our experiments showed that:
- Bag-of-Words and TF-IDF methods had higher accuracy compared to Word2Vec.
- StandardScaler did not improve performance for BoW and TF-IDF and actually reduced accuracy.
- Despite lower accuracy, Word2Vec gives valuable insights into the semantic relationships between words.

These findings help us understand the strengths and weaknesses of each text representation method for classifying game reviews. Future work can involve trying other text representation techniques and improving preprocessing steps to make the models better.

### Final Thoughts

The project gave us a good understanding of what makes a game successful or not based on user reviews. By combining sentiment analysis with advanced text mining techniques, we were able to find meaningful conclusions that could help game developers and other people in the gaming industry.

</article>